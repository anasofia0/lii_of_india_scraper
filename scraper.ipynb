{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all law case links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search queries\n",
    "queries = ['forced labour',\n",
    "            'forced labor',\n",
    "            'coerced labor',\n",
    "            'coerced labour',\n",
    "            'labor exploitation',\n",
    "            'labour exploitation',\n",
    "            'labour bondage',\n",
    "            'labor bondage',\n",
    "            'involuntary servitude',\n",
    "            'human traffickingâ€‹',\n",
    "            'child labor',\n",
    "            'child labour',\n",
    "            'modern slavery',\n",
    "            'slavery']\n",
    "\n",
    "# required header\n",
    "header = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:128.0) Gecko/20100101 Firefox/128.0',\n",
    "          'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8'\n",
    "          }\n",
    "\n",
    "domain = 'http://www.liiofindia.org/'\n",
    "\n",
    "# base query template\n",
    "query_base = 'http://www.liiofindia.org/cgi-bin/sinosrch.cgi?method=boolean&query={}&meta=%2Fliiofindia&lii=LIIofIndia&mask_path=in%2Fcases'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_law_urls(soup):\n",
    "    \"\"\"\n",
    "    Get all law case urls from search page\n",
    "    \"\"\"\n",
    "    law_cases_url = []\n",
    "    law_cases_list = soup.find('ol')\n",
    "\n",
    "    # some queries return empty response\n",
    "    if law_cases_list == None:\n",
    "        return None\n",
    "    \n",
    "    law_cases_list = law_cases_list.find_all('li')\n",
    "    \n",
    "    for case in law_cases_list:\n",
    "        for item in case.p.contents:\n",
    "            if item.name == 'a':\n",
    "                url_case = item['href']\n",
    "                title = item.contents[0]\n",
    "                year = [int(y) for y in re.findall(r'\\d{4}', title)]\n",
    "                year = min(year) if year != [] else None\n",
    "            elif item.name == 'small':\n",
    "                database = item.contents[1].contents[0]\n",
    "        \n",
    "        # check if its law case\n",
    "        if re.search(r'\\s[vV][sS]?\\.?\\s', title):\n",
    "            law_cases_url.append({'url': url_case,\n",
    "                                'title': title,\n",
    "                                'year': year,\n",
    "                                'database': database})\n",
    "    return law_cases_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "law_cases = []\n",
    "for query in queries:\n",
    "    query_cases = []\n",
    "    url = query_base.format(query.replace(' ', '+'))\n",
    "    soup = BeautifulSoup(requests.get(url, headers=header).content, \"html.parser\")\n",
    "\n",
    "    search_pages = []\n",
    "    for url in soup.find_all('a'):\n",
    "        if url.has_attr('href') and re.match(r'.*offset=\\d+0.*', url['href']):\n",
    "            search_pages.append(url['href'])\n",
    "    search_pages = ['http://www.liiofindia.org' + page for page in list(set(search_pages)) if 'http://www.liiofindia.org' not in page]\n",
    "\n",
    "    # search links in first page\n",
    "    urls = get_law_urls(soup)\n",
    "    if urls != None:\n",
    "        query_cases += urls\n",
    "\n",
    "        # search in offset pages\n",
    "        for page in search_pages:\n",
    "            soup = BeautifulSoup(requests.get(page, headers=header).content, \"html.parser\")\n",
    "            query_cases += get_law_urls(soup)\n",
    "\n",
    "        # update query value\n",
    "        for idx, case in enumerate(query_cases):\n",
    "            query_cases[idx]['query'] = query\n",
    "        \n",
    "        # add to law cases list\n",
    "        law_cases += query_cases\n",
    "    \n",
    "    print(f'{query}: {len(query_cases)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(law_cases)\n",
    "df.to_csv('search.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('search.csv')\n",
    "df['content'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(soup):\n",
    "    soup.find_all('h2')\n",
    "    title = soup.h2\n",
    "    document = title.find_next('p')\n",
    "\n",
    "    # remove footnote\n",
    "    document = re.sub(r'\\[\\nContext\\n\\] \\[\\nHide Context\\n\\]\\n(?:CommonLII|LIIofIndia):\\nCopyright Policy\\n\\|\\nDisclaimers\\n\\|\\nPrivacy Policy\\n\\|\\nFeedback\\nURL:\\s.*', '', document.get_text(strip=True, separator='\\n'))\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    if not pd.isna(row.content): continue\n",
    "    sleep(0.01)\n",
    "    soup = BeautifulSoup(requests.get(row.url, headers=header).content, \"html.parser\")\n",
    "\n",
    "    # check if it is embbeded pdf\n",
    "    obj = soup.find('object')\n",
    "    if obj is not None and obj['type'] == 'application/pdf':\n",
    "        df.loc[idx, 'content'] = ''\n",
    "        continue\n",
    "\n",
    "    content = get_content(soup)\n",
    "    df.loc[idx, 'content'] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('lii_india_modern_slavery.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.loc[df['content']==''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Embedded PDFs and extract their content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lii_india_modern_slavery.csv')\n",
    "pdf_df = df[df.isnull().any(axis=1)]\n",
    "pdf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_downloaded = -1 # checkpoint\n",
    "data_path = './data/' # download path\n",
    "downloaded_files = [] # list of downloaded files with corresponding rows\n",
    "prev_downloaded = [file.split('/')[-1] for file in os.listdir(data_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for idx, row in tqdm(pdf_df.iterrows(), total=pdf_df.shape[0]):\n",
    "    if idx <= last_downloaded: continue\n",
    "    sleep(0.01)\n",
    "    soup = BeautifulSoup(requests.get(row.url, headers=header).content, \"html.parser\")\n",
    "    \n",
    "    obj = soup.find('object')\n",
    "    if obj is not None and obj['type'] == 'application/pdf':\n",
    "        file_path = obj['data'] # get file location in database\n",
    "        filename = file_path.replace('/', '_').strip('_').replace('in_cases_', '')\n",
    "        if filename not in prev_downloaded:\n",
    "            sleep(0.01)\n",
    "            content = requests.get(domain+file_path, headers=header).content # get file from database\n",
    "            with open(data_path+filename, 'wb') as f:\n",
    "                f.write(content)\n",
    "        downloaded_files.append((idx, filename)) # save idx and filename\n",
    "    elif 'There is no available HTML version of this document' in soup.get_text():\n",
    "        if file_path := re.search(r'(in\\/cases\\/.*\\.html)', row.url):\n",
    "            file_path = file_path.group(1).replace('html', 'pdf')\n",
    "            filename = file_path.replace('/', '_').strip('_').replace('in_cases_', '')\n",
    "            if filename not in prev_downloaded:\n",
    "                sleep(0.01)\n",
    "                content = requests.get(domain+file_path, headers=header).content # get file from database\n",
    "                with open(data_path+filename, 'wb') as f:\n",
    "                    f.write(content)\n",
    "            downloaded_files.append((idx, filename)) # save idx and filename\n",
    "    else:\n",
    "        downloaded_files.append((idx, -1)) # save idx and -1 if download file not found\n",
    "\n",
    "    last_downloaded = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(downloaded_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list = [idx for idx, _ in downloaded_files]\n",
    "doc_content = []\n",
    "for idx, file in downloaded_files:\n",
    "    if file == -1:\n",
    "        doc_content.append('')\n",
    "    else:\n",
    "        doc = pymupdf.open(data_path+file)\n",
    "        content = ' '.join([page.get_text() for page in doc])\n",
    "        doc_content.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_list, 'content'] = doc_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('lii_india_modern_slavery.csv', index=False)\n",
    "df.to_excel('lii_india_modern_slavery.xlsx', sheet_name='lii_india_modern_slavery', engine='xlsxwriter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapper",
   "language": "python",
   "name": "scrapper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
